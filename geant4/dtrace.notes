== How to profile with DTrace

.What should be the entry point for our profiling ?

We would like to skip the whole initialization part of +full_cms+, therefore one
shoule trace from +BeamOn()+ or even better from the +EventManager::DoEventLoop()+,
because +BeamOn()+ calls some initialization routines itself. For this purpose,
we use a predicate as shown in +stack2dtrace.sh+ file.

.Which functions should we be profiling ?

Trying to calculate the time that each functions consumes is troublesome. DTrace matches around
+30,000+ probes and the execution is slowed down considerably. If that option is chosen though,
use +vtimestamp+ instead of +timestamp+, so that the results are less affected by the DTrace itself.

Another, more pragmatic approach, is to identify those functions that contribute most in the overall
execution time and focus the profiling on them only. This can be done by taking sample stack traces,
while +full_cms+ is running. Then, we examine those stack traces and deduce which functions are the 
"hottest" (a function that is taking a lot time to execute, is more likely to get sampled, since we
are sampling at a fixed rate).

More information can be found https://github.com/brendangregg/FlameGraph[here]. E.g.,
 
----
solaris# dtrace -x ustackframes=100								  \
	 	-x strsize=5000									  \
		-n 'BEGIN { tracing = 0; }							  \
		    pid$target::*DoEventLoop*:entry { tracing = 1; }				  \
		    pid$target::*DoEventLoop*:return { exit(0); }				  \
		    profile-97/tracing != 0/ { @[ustack()] = count(); }'			  \
		 -c "/home/stathis/geant4/geant4.9.5.p01/bin/Linux-g++/full_cms ./bench_10.g4" 	  \
		 -o stack
----

IMPORTANT: The profile target does NOT fire in thread context, therefore using +self->tracing+
as a predicate will not work. Use a global variable instead.

For 10 events the above invocation produces approximately 3MB of profiling data.

.Next steps are:

* pass the output to +cxxfilt+ to demangle the c++ symbols (optional)
* generate the folded output with +stackcollapse.pl+
* delete the line corresponding the idle line, which looks like +"  4168"+
* generate the svg file with +flamegraph.pl+

http://leaf.dragonflybsd.org/~beket/geant4/flamegraph.svg[This] is an example of the svg output
file, and http://leaf.dragonflybsd.org/~beket/geant4/flamegraph-demangled.svg[this] is with
demangled names.

.Generate the targeted DTrace script

After having identified the "hot" functions, we run a shell script that generates a DTrace
script that focuses on profiling these functions only.

----
curl -o stacks2dtrace.sh http://leaf.dragonflybsd.org/~beket/geant4/stack2dtrace.sh
chmod +x stacks2dtrace.sh
stacks2dtrace.sh <folded.file> > test.d
----

Example of DTrace script:
----
curl -o test.d http://leaf.dragonflybsd.org/~beket/geant4/test.d
----

The script should be called with +-Z+ (for not failing on unmathced probes) and with
+-x dynvarsize=8m+ (for not dropping dynamic variables). E.g.,

----
solaris# ./test.d -Z -x dynvarsize=8m \
	 -c "/home/stathis/gen-tests/geant4/geant4.9.5.p01/bin/Linux-g++/full_cms ./bench1_10.g4"
----

Example of DTrace output:
----
curl -o http://leaf.dragonflybsd.org/~beket/geant4/test.d.out1 \
     c++filt -n -p
----

NOTE: g\++ generates mangled symbols during the compilation. So, we use \*globs* inside the DTrace script,
if we don't want to nm(1) the executable and copy the exact (mangled) names from there. I looked
for a way to generate mangled names for g\++, but their abi:: exported interface has only +__cxa_demangle()+.

.How many events should we be simulating ?

We let the +full_cms+ run for +100+ events and every time one such event is processed,
we print the average time spent on the "hot" functions. The idea is to see past which
event region the average times enter a steady state.  We do this by enabling a probe,
like the following:

----
pid$target::*MyEventAction*EndOfEventAction*:entry
{
	printa(@_);
}
----

An example can be found http://leaf.dragonflybsd.org/~beket/geant4/stad.out[here]. We, then, use
a http://leaf.dragonflybsd.org/~beket/geant4/q.awk[script file] to extract the aggregations for
every event, and http://leaf.dragonflybsd.org/~beket/geant4/rec.sh[another one], to reconstruct
a time series for every "hot" function.

----
(echo -n 'set log y; plot '; for f in _Z*; do echo -n "'$f' notitle with lines,"; done) \
      | gnuplot -persist
(echo -n 'plot '; for f in _Z*; do echo -n "'$f' notitle smooth csplines,"; done) \
      | gnuplot -persist
----

Complementary, we generate separate plots for each one of the traced functions, with the use
of http://leaf.dragonflybsd.org/~beket/geant4/genpl.sh[this script]. Sample output can be
found either in the form of a https://plus.google.com/u/0/photos/108524938275387430003/albums/5750217012217829393[picassa web album]
or as a http://leaf.dragonflybsd.org/~beket/geant4/plots/bench1_100.g4.min30[plain directory listing].

== How to use other metrics, such as cycles per instruction (CPI)

Solaris has two utilities for that: +cpustat+ for overall system monitoring, and +cputrack+ for
monitoring a process (or a family of them). With the help of http://gitweb.dragonflybsd.org/~beket/gen-tests.git/blob/refs/heads/master:/geant4/x64ipc.sh[this script] we can measure
the cycles per instruction (CPI) of the +full_cms+ experiment. For example:

----
solaris# ./x64ipc.sh ./full_cms ./bench1_10.g4 
    Instructions      CPI     %CPU
        71986399     1.13    65.42
           69678     2.00     9.69
         3035697     1.22    17.47
           21070     3.13     7.21
         2479098     1.22    18.21
           21119     3.31     7.09
         1684096     1.15    18.34
           21170     3.20     7.05
         1117784     1.06    18.38
^C
----

A file with full results from a simulation of the +bench1_100.g4+ can be found
http://leaf.dragonflybsd.org/~beket/geant4/bench1_100.g4.cpi.dat[here]. The results can be plotted
with http://gitweb.dragonflybsd.org/~beket/gen-tests.git/blob/refs/heads/master:/geant4/cpiplot.sh[this script]. See also the http://leaf.dragonflybsd.org/~beket/geant4/bench1_100.g4.cpi.dat.png[graphic output] from the previous example.

The part of the plot, where the CPI peaks, is probably contributed by the initialization of +full_cms+. As we enter the actual simulation, CPI stabilizes.

NOTE: Rule of thumb: Low CPI: we are cpu-bounded, High CPI: we are memory-bounded (cache misses?).
NOTE TO SELF: Also +cputrack+ has -e -f options.

*References*: http://dtrace.org/blogs/brendan/2007/02/27/amd64-pics-cpi/[AMD64 PICs, CPI], by Brendan Gregg
